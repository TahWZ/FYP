{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import csv\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from scipy.io import arff\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn import metrics\r\n",
    "import math\r\n",
    "\r\n",
    "# sys.path.append('interface')\r\n",
    "\r\n",
    "# Allows jupyter notebook to be imported\r\n",
    "import jupyter_import\r\n",
    "\r\n",
    "# Suppress Warnings\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from data_preproc.Preprocess import preprocess, Normalize"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "importing Jupyter notebook from D:\\FYP i guess\\I need to do Naive Bayes\\Program\\Algorithm\\data_preproc\\Preprocess.ipynb\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Selection"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from data_preproc.CFS import cfs_algo\r\n",
    "from data_preproc.RFE import rfe_algo\r\n",
    "from data_preproc.RR import ridge_algo"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "importing Jupyter notebook from D:\\FYP i guess\\I need to do Naive Bayes\\Program\\Algorithm\\data_preproc\\CFS.ipynb\n",
      "importing Jupyter notebook from D:\\FYP i guess\\I need to do Naive Bayes\\Program\\Algorithm\\data_preproc\\RFE.ipynb\n",
      "importing Jupyter notebook from D:\\FYP i guess\\I need to do Naive Bayes\\Program\\Algorithm\\data_preproc\\RR.ipynb\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Algorithms"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Base Predictors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from pred_mdls.base.Complement_Naive_Bayes import complement_naive_bayes_model\r\n",
    "from pred_mdls.base.Decision_Tree import decision_tree_model\r\n",
    "from pred_mdls.base.Logistic_Regression import logistic_regression_model\r\n",
    "from pred_mdls.base.Multi_Layer_Perceptron import multi_layer_perceptron_model\r\n",
    "from pred_mdls.base.Naive_Bayes import naive_bayes_model"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "importing Jupyter notebook from D:\\FYP i guess\\I need to do Naive Bayes\\Program\\Algorithm\\pred_mdls\\base\\Complement_Naive_Bayes.ipynb\n",
      "importing Jupyter notebook from D:\\FYP i guess\\I need to do Naive Bayes\\Program\\Algorithm\\pred_mdls\\base\\Decision_Tree.ipynb\n",
      "importing Jupyter notebook from D:\\FYP i guess\\I need to do Naive Bayes\\Program\\Algorithm\\pred_mdls\\base\\Logistic_Regression.ipynb\n",
      "importing Jupyter notebook from D:\\FYP i guess\\I need to do Naive Bayes\\Program\\Algorithm\\pred_mdls\\base\\Multi_Layer_Perceptron.ipynb\n",
      "importing Jupyter notebook from D:\\FYP i guess\\I need to do Naive Bayes\\Program\\Algorithm\\pred_mdls\\base\\Naive_Bayes.ipynb\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ensemble Predictors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from pred_mdls.ensemble.Random_Forest import random_forest_model\r\n",
    "from pred_mdls.ensemble.Rotation_Forest import rotation_forest_model\r\n",
    "from pred_mdls.ensemble.Voting import voting_model"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "importing Jupyter notebook from D:\\FYP i guess\\I need to do Naive Bayes\\Program\\Algorithm\\pred_mdls\\ensemble\\Random_Forest.ipynb\n",
      "importing Jupyter notebook from D:\\FYP i guess\\I need to do Naive Bayes\\Program\\Algorithm\\pred_mdls\\ensemble\\Rotation_Forest.ipynb\n",
      "importing Jupyter notebook from D:\\FYP i guess\\I need to do Naive Bayes\\Program\\Algorithm\\pred_mdls\\ensemble\\Voting.ipynb\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation Metrics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from pf_eval.AUC_ROC import auc_roc_model\r\n",
    "from pf_eval.F1_Score import f1_model\r\n",
    "from pf_eval.CSV import write_results\r\n",
    "from pf_eval.Confusion_Matrix import confusion_matrix_model"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "importing Jupyter notebook from D:\\FYP i guess\\I need to do Naive Bayes\\Program\\Algorithm\\pf_eval\\AUC_ROC.ipynb\n",
      "importing Jupyter notebook from D:\\FYP i guess\\I need to do Naive Bayes\\Program\\Algorithm\\pf_eval\\F1_Score.ipynb\n",
      "importing Jupyter notebook from D:\\FYP i guess\\I need to do Naive Bayes\\Program\\Algorithm\\pf_eval\\CSV.ipynb\n",
      "importing Jupyter notebook from D:\\FYP i guess\\I need to do Naive Bayes\\Program\\Algorithm\\pf_eval\\Confusion_Matrix.ipynb\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Additional Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def data_conversion(data):\r\n",
    "    for i in range(len(data)):\r\n",
    "        if data[i] == b'N' or data[i] == b'false' or data[i] == b'no':\r\n",
    "            data[i] = 0\r\n",
    "        else:\r\n",
    "            data[i] = 1\r\n",
    "    return data\r\n",
    "\r\n",
    "def read_data(filename):\r\n",
    "    data = arff.loadarff(filename)\r\n",
    "    loaddata = pd.DataFrame(data[0])\r\n",
    "    return loaddata\r\n",
    "\r\n",
    "def process_data(loaddata,features):\r\n",
    "    # Features are selected based on CFS\r\n",
    "    software_metrics = np.array(loaddata[features])\r\n",
    "    labels = np.array(loaddata['Defective'])\r\n",
    "    return software_metrics,labels\r\n",
    "\r\n",
    "def train_data(software_metrics,labels):\r\n",
    "    X_train, X_test, y_train, y_test = train_test_split(software_metrics, labels, test_size = 0.1)\r\n",
    "    y_train = y_train.astype('str')\r\n",
    "    y_test = y_test.astype('str')\r\n",
    "    return X_train, X_test, y_train, y_test\r\n",
    "\r\n",
    "def evaluate_data(model,X_test,y_test):\r\n",
    "    auc_score = auc_roc_model(model,X_test,y_test)\r\n",
    "    f1_score = f1_model(model,X_test,y_test)\r\n",
    "    fpr,fnr = confusion_matrix_model(model,X_test,y_test)\r\n",
    "    return auc_score,f1_score,fpr,fnr\r\n",
    "\r\n",
    "def translate(result):\r\n",
    "    count = 1\r\n",
    "    res = []\r\n",
    "    while count <= 3:\r\n",
    "        for i in range(len(result[0])):\r\n",
    "            res.append([result[0][i], result[1][((i+1)*count)-1],result[2][((i+1)*count)-1]])\r\n",
    "        count += 1\r\n",
    "    return res"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Result writers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def main_writer(header,result):\r\n",
    "    #Writes the output of a single dataset for main function\r\n",
    "    filters = ['No filter','CFS','RFE']\r\n",
    "    with open('pred_results.csv','w',encoding='UTF8', newline='') as file:\r\n",
    "        res = csv.writer(file)\r\n",
    "        for i in range(len(filters)):\r\n",
    "            res.writerow('')\r\n",
    "            res.writerow([filters[i]])\r\n",
    "            res.writerow(header)\r\n",
    "            res.writerow([result[0][0]] + result[0][1][i*8:i*8+8])\r\n",
    "            res.writerow([result[1][0]] + result[1][1][i*8:i*8+8])\r\n",
    "    \r\n",
    "# def run(datasets, savename, repository):\r\n",
    "def run(datasets, savename, results, model_name, pp_name):\r\n",
    "    #Writes the output of multiple datasets for the main function\r\n",
    "    # header = ['Model name','Complement Naive Bayes','Decision Tree','Logistic regression',\r\n",
    "    #                     'Multi Layer Perceptron','Naive Bayes','Random Forest','Rotation Forest','Voting']\r\n",
    "    header = ['Model name'] + model_name\r\n",
    "    # filters = ['(All)','(CFS)','(RFE)']\r\n",
    "    n = len(model_name)\r\n",
    "    with open('csv_results/' + savename + '.csv','w',encoding='UTF8', newline='') as csv_file:\r\n",
    "        res = csv.writer(csv_file)\r\n",
    "        for k in range(len(results[0])):\r\n",
    "            # AUC, F1, FPR, FNR\r\n",
    "            res.writerow([results[0][k][0]])\r\n",
    "            # Model Name\r\n",
    "            res.writerow(header)\r\n",
    "            for j in range(len(results)):\r\n",
    "                col_num = 0\r\n",
    "                for i in range(len(pp_name[j])):\r\n",
    "                    res.writerow([f'{datasets[j]} ({pp_name[j][i]})'] + results[j][k][1][col_num:col_num+n])\r\n",
    "                    col_num += n\r\n",
    "            if k != len(results[0])-1:\r\n",
    "                res.writerow('')\r\n",
    "    # with open(savename,'w',encoding='UTF8', newline='') as file:\r\n",
    "        # results = []\r\n",
    "        # for ds in datasets:\r\n",
    "        #     if repository == 'NASA':\r\n",
    "        #         results.append(main('datasets/NASA/' + ds + '.txt')[0])\r\n",
    "        #     else:\r\n",
    "        #         results.append(main('datasets/PROMISE/' + ds + '.txt')[0])\r\n",
    "        # res = csv.writer(file)\r\n",
    "        # for k in range(len(results[0])):\r\n",
    "        #     res.writerow([results[0][k][0]])\r\n",
    "        #     res.writerow(header)\r\n",
    "        #     for j in range(len(results)):\r\n",
    "        #         for i in range(len(filters)):\r\n",
    "        #             res.writerow([datasets[j]+filters[i]] + results[j][k][1][i*8:i*8+8])\r\n",
    "        #     if k != len(results[0])-1:\r\n",
    "        #         res.writerow('')\r\n",
    "\r\n",
    "        #res.writerow([results[0][0][0]])\r\n",
    "        #res.writerow(header)        \r\n",
    "        #for j in range(len(results)):\r\n",
    "            #for i in range(len(filters)):\r\n",
    "                #res.writerow([datasets[j]+filters[i]] + results[j][0][1][i*8:i*8+8])\r\n",
    "        #res.writerow('')\r\n",
    "        #res.writerow([results[0][1][0]])\r\n",
    "        #res.writerow(header)\r\n",
    "        #for j in range(len(results)):\r\n",
    "            #for i in range(len(filters)):\r\n",
    "                #res.writerow([datasets[j]+filters[i]] + results[j][1][1][i*8:i*8+8])\r\n",
    "        #res.writerow('')\r\n",
    "        #res.writerow([results[0][2][0]])\r\n",
    "        #res.writerow(header)\r\n",
    "        #for j in range(len(results)):\r\n",
    "            #for i in range(len(filters)):\r\n",
    "                #res.writerow([datasets[j]+filters[i]] + results[j][2][1][i*8:i*8+8])\r\n",
    "        #res.writerow('')\r\n",
    "        #res.writerow([results[0][3][0]])\r\n",
    "        #res.writerow(header)\r\n",
    "        #for j in range(len(results)):\r\n",
    "            #for i in range(len(filters)):\r\n",
    "                #res.writerow([datasets[j]+filters[i]] + results[j][3][1][i*8:i*8+8])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def main_algo_run(filename,fs_res,pred_res,train_res):\r\n",
    "    # Read the file\r\n",
    "    loaddata = read_data(filename)\r\n",
    "    loaddata = Normalize(loaddata)\r\n",
    "    SM = np.array(loaddata.iloc[:,:-1]) #Software metrics\r\n",
    "    L = data_conversion(np.array(loaddata.iloc[:,-1])).astype(int) #Labels\r\n",
    "    data = [SM,L]\r\n",
    "    lookup_model = ['Complement Naive Bayes','Decision Tree','Logistic regression','Multi Layer Perceptron','Naive Bayes',\r\n",
    "            'Random Forest','Rotation Forest','Voting'] #Models used\r\n",
    "    lookup_pp = ['All','CFS','RFE']\r\n",
    "    # selection = \"\"\r\n",
    "    # while selection.strip().replace(\" \",\"\").isdigit() == False:\r\n",
    "    #     selection = input(\"Please select which models you would like to use by inputting the numbers specified beside them.\\n\" \r\n",
    "    #     \"To make multiple selections, seperate the numbers by spaces.\\n\" \r\n",
    "    #     \"1. Complement Naive Bayes\\n\"  \r\n",
    "    #     \"2. Decision Tree\\n\" \r\n",
    "    #     \"3. Logistic regression\\n\" \r\n",
    "    #     \"4. Multi Layer Perceptron\\n\"  \r\n",
    "    #     \"5. Naive Bayes\\n\"  \r\n",
    "    #     \"6. Random Forest\\n\"  \r\n",
    "    #     \"7. Rotation Forest\\n\" \r\n",
    "    #     \"8. Voting\\n\")\r\n",
    "    # selection = selection.strip().replace(\" \",\"\")\r\n",
    "    # selection = sorted(selection)\r\n",
    "    # model_name = []\r\n",
    "    # for i in selection:\r\n",
    "    #     model_name.append((model_name[int(i)-1],model_name.index(model_name[int(i)-1])))\r\n",
    "    # print(model_name)\r\n",
    "    # ===== Feature Selection ====== #\r\n",
    "\r\n",
    "    fs_arr = [i for i in range(len(fs_res)) if fs_res[i]]\r\n",
    "\r\n",
    "    pp_name = [lookup_pp[i] for i in fs_arr]\r\n",
    "\r\n",
    "    def feature_selection(fs_res,loaddata,data,train_size,k_fold):\r\n",
    "        pp_arr = []\r\n",
    "        feature_funcs = [cfs_algo,rfe_algo]\r\n",
    "        if fs_res[0]:\r\n",
    "            pp_arr.append(preprocess(loaddata,k_fold))\r\n",
    "        for i in range(1,len(fs_res)):\r\n",
    "            if fs_res[i]:\r\n",
    "                _,f_selection = feature_funcs[i-1](data,train_size)\r\n",
    "                pp_arr.append(preprocess(loaddata,k_fold,f_selection))\r\n",
    "        return pp_arr\r\n",
    "\r\n",
    "    train_size = int(train_res['tt']) if not train_res['tt'] == '' else 10\r\n",
    "\r\n",
    "    k_fold = int(train_res['kfold']) if not train_res['kfold'] == '' else 5\r\n",
    "\r\n",
    "    pp_arr = feature_selection(fs_res,loaddata,data,train_size,k_fold)\r\n",
    "            \r\n",
    "    # # ==== CFS ==== #\r\n",
    "    # cfs, cfs_selections = cfs_algo(data,10)\r\n",
    "    # # ============= #\r\n",
    "\r\n",
    "    # # ===== RFE ======== #\r\n",
    "    # rfe, rfe_selections = rfe_algo(data,10)\r\n",
    "    # # ================== #\r\n",
    "    \r\n",
    "    # # ========= Preprocessing ============= #\r\n",
    "    # pp = preprocess(loaddata)\r\n",
    "    # pp_cfs = preprocess(loaddata, cfs_selections)\r\n",
    "    # pp_rfe = preprocess(loaddata, rfe_selections)\r\n",
    "\r\n",
    "    # pp_arr = [pp,pp_cfs,pp_rfe]\r\n",
    "    # pp_name = ['No filters','CFS Feature Selection','RFE Feature Selection']\r\n",
    "    # ====================================== #\r\n",
    "\r\n",
    "    base_preds = [i for i,pred in enumerate(pred_res['base']) if pred == 1]\r\n",
    "\r\n",
    "    ensemble_preds = [i for i,pred in enumerate(pred_res['ensemble']) if pred == 1]\r\n",
    "\r\n",
    "    model_name = [lookup_model[index] for index in base_preds] + [lookup_model[index+5] for index in ensemble_preds]\r\n",
    "    \r\n",
    "    length_preds = len(base_preds) + len(ensemble_preds)\r\n",
    "\r\n",
    "    def model_creation(base_preds,ensemble_preds,data):\r\n",
    "        models = []\r\n",
    "        args = [1000]\r\n",
    "        base_funcs = [\r\n",
    "            complement_naive_bayes_model,\r\n",
    "            decision_tree_model,\r\n",
    "            logistic_regression_model,\r\n",
    "            multi_layer_perceptron_model,\r\n",
    "            naive_bayes_model \r\n",
    "        ]\r\n",
    "        ensemble_funcs = [\r\n",
    "            random_forest_model,\r\n",
    "            rotation_forest_model,\r\n",
    "            voting_model\r\n",
    "        ]\r\n",
    "        for index in base_preds:\r\n",
    "            models.append(base_funcs[index](data))\r\n",
    "        for index in ensemble_preds:\r\n",
    "            models.append(ensemble_funcs[index](data,args))\r\n",
    "        return models\r\n",
    "\r\n",
    "    result = []\r\n",
    "    arr_size = length_preds*len(pp_name) #Result array size\r\n",
    "    auc_arr = [0]*arr_size\r\n",
    "    f1_arr = [0]*arr_size\r\n",
    "    fpr_arr = [0]*arr_size\r\n",
    "    fnr_arr = [0]*arr_size\r\n",
    "    header = []\r\n",
    "    folds = 5\r\n",
    "    for j,pp in enumerate(pp_arr):\r\n",
    "        for i in range(folds):\r\n",
    "            data = [pp[i][0],pp[i][2]]\r\n",
    "            \r\n",
    "            # # ======== Model Creation =========== #\r\n",
    "            # # Base Predictors\r\n",
    "            # cnb = complement_naive_bayes_model(data)\r\n",
    "            # dt = decision_tree_model(data)\r\n",
    "            # lr = logistic_regression_model(data)\r\n",
    "            # mlp = multi_layer_perceptron_model(data)\r\n",
    "            # nb = naive_bayes_model(data)\r\n",
    "\r\n",
    "            # args = [1000]\r\n",
    "            # # Ensemble Predictors\r\n",
    "            # rf = random_forest_model(data,args)\r\n",
    "            # rof = rotation_forest_model(data,args)\r\n",
    "            # vt = voting_model(data,args)\r\n",
    "            # ==================================== #\r\n",
    "            # models = [cnb,dt,lr,mlp,nb,rf,rof,vt]\r\n",
    "            # used_models = []\r\n",
    "            # for x in selection:\r\n",
    "            #     used_models.append(models[int(x)-1])\r\n",
    "\r\n",
    "            models = model_creation(base_preds,ensemble_preds,data)\r\n",
    "            \r\n",
    "            for k in range(len(models)):\r\n",
    "                auc_score,f1_score,fpr,fnr = evaluate_data(models[k],pp[i][1],pp[i][3])\r\n",
    "                if math.isnan(auc_score):\r\n",
    "                    #print(model_name[k], auc_score)\r\n",
    "                    auc_score = 0\r\n",
    "                auc_arr[(j*len(model_name))+k] += auc_score\r\n",
    "                f1_arr[(j*len(model_name))+k] += f1_score\r\n",
    "                fpr_arr[(j*len(model_name))+k] += fpr\r\n",
    "                fnr_arr[(j*len(model_name))+k] += fnr\r\n",
    "\r\n",
    "    for i in range(len(auc_arr)):\r\n",
    "        auc_arr[i] /= folds\r\n",
    "        auc_arr[i] = round(auc_arr[i],3)\r\n",
    "        f1_arr[i] /= folds\r\n",
    "        f1_arr[i] = round(f1_arr[i],3)\r\n",
    "        fpr_arr[i] /= folds\r\n",
    "        fpr_arr[i] = round(fpr_arr[i],3)\r\n",
    "        fnr_arr[i] /= folds\r\n",
    "        fnr_arr[i] = round(fnr_arr[i],3)\r\n",
    "    header.append('Model Name')\r\n",
    "    for i in model_name:\r\n",
    "        header.append(i[0]) \r\n",
    "    result.append(('AUC', auc_arr))\r\n",
    "    result.append(('F1 Score', f1_arr))\r\n",
    "    result.append(('False Positive Rate', fpr_arr))\r\n",
    "    result.append(('False Negative Rate', fnr_arr))\r\n",
    "    return model_name,pp_name,result\r\n",
    "    #Print filename upon completion\r\n",
    "    # print(filename)\r\n",
    "    # return result, header\r\n",
    "      \r\n",
    "if __name__=='__main__':\r\n",
    "    N_filenames = ['CM1.arff','JM1.arff','KC1.arff','KC3.arff',\r\n",
    "                   'KC4.arff','MC1.arff','MC2.arff','MW1.arff',\r\n",
    "                   'PC1.arff','PC2.arff','PC3.arff','PC4.arff','PC5.arff']\r\n",
    "    P_filenames = ['cm1.arff','jm1.arff','kc1.arff','kc2.arff','pc1.arff']\r\n",
    "    run(N_filenames,'NASA.csv','NASA')\r\n",
    "    run(P_filenames,'PROMISE.csv','PROMISE')\r\n",
    "    #========== Running main program =========#\r\n",
    "    result, header = main_algo_run('datasets/NASA/CM1.arff.txt')\r\n",
    "    main_writer(header,result)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Please select which models you would like to use by inputting the numbers specified beside them.\n",
      "To make multiple selections, seperate the numbers by spaces.\n",
      "1. Complement Naive Bayes\n",
      "2. Decision Tree\n",
      "3. Logistic regression\n",
      "4. Multi Layer Perceptron\n",
      "5. Naive Bayes\n",
      "6. Random Forest\n",
      "7. Rotation Forest\n",
      "8. Voting\n",
      "1 3 2\n",
      "[('Complement Naive Bayes', 0), ('Decision Tree', 1), ('Logistic regression', 2)]\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "datasets/NASA/CM1.arff.txt\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d46af94c2bbce495f1e668725902fa517c90b1782bcfe2fce0dd9868df553d3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}